# Ralph Progress Log
Started: Fri 13 Feb 2026 18:47:44 CST
---

## Codebase Patterns
- CLI entry is `packages/opencode/src/index.ts` — yargs middleware runs before any command
- Bootstrap phases are in `packages/opencode/src/project/bootstrap.ts` — already has `measure()` helper
- Instance creation flow: `Instance.provide()` → `Project.fromDirectory()` → `init()` (InstanceBootstrap)
- Config loading is in `Config.state()` (lazy per-Instance) with sub-phases: auth, remote-fetch, global, project, directory-scan
- Worker thread entry: `packages/opencode/src/cli/cmd/tui/worker.ts` — `Log.init()` is now synchronous, followed by `await Log.flush()`
- `Log.init()` is synchronous — buffers to in-memory array. Call `Log.flush()` after directories are ready to write buffer to disk
- TUI init: `packages/opencode/src/cli/cmd/tui/app.tsx` — `getTerminalBackgroundColor()` now uses COLORFGBG/cache/150ms timeout
- KV store file is at `~/.local/state/opencode/kv.json` — can be read directly outside SolidJS context via `Bun.file().json()`
- `packages/opencode/src/global/index.ts` — no module-level await; call `Global.ensureDirectories()` explicitly before `Log.flush()`
- Pre-existing 6 security test failures in `write-edit-bypass.test.ts`, `audit-log.test.ts`, `inheritance.test.ts`, `config.test.ts`, `mcp-policy.test.ts` — not related to cold-start work
- TUI thread (`thread.ts`) now starts TUI with RPC immediately; resolveNetworkOptions runs in background. HTTP server only started if CLI flags or config require it
- Remote wellknown config fetched with 3s timeout, cached to `Global.Path.cache/wellknown/` with 5-min TTL (stale-while-revalidate)
- Plugin loading (`plugin/index.ts`) uses `Promise.allSettled()` for both internal and external plugins; `BunProc.install()` has Lock serialization + 10s timeout
- `installDependencies()` runs in background after `Config.state()` returns — `Instance.dispose()` triggers config reload on completion
- `Auth.all()` and `global()` run in parallel at start of `Config.state()` — independent data sources
- `loadCommand/loadAgent/loadMode/loadPlugin` run via `Promise.all()` per directory — different glob patterns, independent subdirectories
- Git rev-parse results cached in `.git/opencode-metadata` (JSON) with `.git/HEAD` mtime invalidation — skips subprocess spawns on warm launch
- InstanceBootstrap parallelization: Group A (Share, ShareNext, Snapshot, Truncate) serial; Group B (Plugin, LSP, Format, Security) + Group C (FileWatcher, File, Vcs) concurrent via nested Promise.all
- Plugin.trigger() is ONLY called at session runtime (18 sites across 8 files) — NOT during bootstrap init functions
- Bun compile dynamic import: static string `import("@ai-sdk/anthropic")` WORKS; variable `import(pkg)` FAILS. Use `Map<string, () => Promise<any>>` registry pattern with arrow functions containing static string imports
- Build uses `Bun.build()` with `compile` option (not CLI `--compile`); bunfig autoload is disabled in production build (`autoloadBunfig: false`)
- LSP.init() and FileWatcher.init() are deferred to background via `queueMicrotask` after bootstrap — not on the critical path. LSP servers only spawn on-demand in `getClients()`
- models-snapshot.ts (~1MB) first import ~45ms — under 50ms threshold, no optimization needed. Already lazy-loaded via `ModelsDev.Data` lazy() factory
- Provider imports: 21 lazy loaders in `BUNDLED_PROVIDER_LOADERS` Map (provider/provider.ts) — each is `async () => (await import("@ai-sdk/pkg")).createFn`. Only loaded when `getSDK()` is first called for that provider. `LanguageModelV2` type imported from `@ai-sdk/provider` (type-only, erased at compile). `AmazonBedrockProviderSettings` uses inline `import()` type syntax. GitLab `VERSION` dynamically imported inside custom loader.
- TUI first-frame critical path: global-init (~0ms) + log-init (~8ms) + worker-spawn (~few ms) + terminal-bg-detect (0ms cached, max 150ms cold) = well under 200ms target
- `--startup-trace --help` and `--startup-trace --version` do NOT output trace (yargs exits before output). Use `--startup-trace models` for non-interactive measurement
- Performance tests live in `test/perf/` — structural verification tests check code invariants, subprocess tests check actual timing
- Subprocess perf tests gated by `OPENCODE_PERF_TEST=1` — structural tests always run. Use `bun test --cwd packages/opencode -- test/perf/` to run all
- Use `test.skipIf(!PERF_ENABLED)` pattern in bun:test for optional CI-gated tests

---

## 2026-02-13 - US-001
- Implemented `--startup-trace` CLI flag with per-phase startup timing output to stderr in JSON format
- Created `packages/opencode/src/util/startup-trace.ts` — StartupTrace module with begin/end/measure/record/output functions supporting nested phases
- Instrumented phases: global-init, log-init, worker-spawn, terminal-bg-detect, resolve-network-options, project-detect, git-metadata, config-load (with sub-phases: remote-fetch, global-config, project-config, directory-scan), server-init, instance-bootstrap (with per-phase detail from existing bootstrap.ts measure)
- Files changed: startup-trace.ts (new), index.ts, bootstrap.ts, instance.ts, project.ts, config.ts, thread.ts, app.tsx
- **Learnings for future iterations:**
  - The StartupTrace module uses a stack-based approach for nested phases — `begin()` pushes a new children array, `end()` pops and nests
  - `record()` is used for flat phase entries (e.g., bootstrap sub-phases that already have their own timing)
  - Config.state() closure requires `??` guards for `result.agent` and `!` for `result.plugin` because TypeScript can't narrow through closures
  - `--startup-trace` flag is checked via `process.argv.includes()` BEFORE yargs parses, so timing starts from first import
  - Output happens after `cli.parse()` completes — for TUI commands that block, trace won't output until TUI exits
---

## 2026-02-13 - US-002
- Optimized terminal background color detection from ~1000ms to ~0ms on subsequent launches
- Three-tier detection: COLORFGBG env var (instant) → KV cache (fast read) → OSC 11 query (150ms timeout)
- Cache stored in `terminal_bg_cache` key in `~/.local/state/opencode/kv.json`
- Exported `clearTerminalBgCache()` from app.tsx for manual re-detection
- Files changed: app.tsx
- **Learnings for future iterations:**
  - COLORFGBG format: "foreground;background" — last value is terminal color index 0-15
  - KV store is a SolidJS context (useKV) — can't use inside getTerminalBackgroundColor, must read file directly
  - `Bun.file().json()` is async — use it for cache reads since the function is already async
  - The KV file may not exist on first launch — handle gracefully with .catch()
---

## 2026-02-13 - US-003
- Made Log.init() synchronous — no longer awaits any IO operations
- Logs buffer to an in-memory array until Log.flush() is called
- Log.flush() writes buffered messages to disk and switches to direct file writer mode
- cleanup() is now fire-and-forget with .catch(() => {}) to prevent unhandled rejections
- fs.truncate() moved into flush() — no longer blocks init()
- When print: true, init() immediately switches to stderr writer (no buffering needed)
- Files changed: log.ts, index.ts, worker.ts, preload.ts, client.test.ts, e2e-local.ts
- **Learnings for future iterations:**
  - `Log.init()` with `print: true` doesn't need flush — writes directly to stderr
  - `Log.init()` with `print: false` buffers to array until `flush()` is called
  - Multiple `Log.init()` calls in test files (after preload) don't re-create writers — they just update level/logpath
  - `flushed` flag prevents double-flush and ensures buffer is only written once
  - The `write` variable is a function reference that gets swapped: buffer → file writer (or stderr)
  - US-005 (Global.ensureDirectories) will further optimize by decoupling directory creation from module-level await, allowing Log.init() + flush() to happen earlier
---

## 2026-02-13 - US-004
- Decoupled resolveNetworkOptions() from TUI rendering — TUI now starts immediately with RPC in the common case
- Split logic into two paths: CLI-explicit server flags (--port/--hostname/--mdns) block as before; common case starts TUI first, resolves config in background
- Background resolution checks if config says server is needed (mdns, non-default port/hostname) and starts HTTP server asynchronously
- TUI works fine over RPC regardless — HTTP server is only for external access
- Files changed: thread.ts
- **Learnings for future iterations:**
  - `SDKProvider` in `sdk.tsx` takes url/fetch/events as props at init time — can't easily change after mount
  - RPC path always works: `createWorkerFetch` → worker `rpc.fetch` → `Server.App().fetch(request)` in-process
  - HTTP server started via `client.call("server", networkOpts)` is additive — doesn't break RPC communication
  - `Config.global()` is the blocking call in `resolveNetworkOptions()` — it's a lazy() that reads config files
  - When CLI flags like `--port` are present, we still need to block to get the actual port value from config before starting the server
---

## 2026-02-13 - US-005
- Eliminated module-level blocking `await` from `global/index.ts`
- Moved directory creation (`fs.mkdir`) and cache version check into `Global.ensureDirectories()` async function
- `Global.Path.*` properties are now pure path computations — no IO on import
- Added `await Global.ensureDirectories()` calls before `await Log.flush()` in: CLI entry (`index.ts`), worker entry (`worker.ts`), test preload (`preload.ts`)
- Cache version check now uses `.catch(() => [])` instead of try/catch for readdir failure (follows codebase no-try-catch style)
- Files changed: global/index.ts, index.ts, worker.ts, preload.ts
- **Learnings for future iterations:**
  - Importing `Global` no longer triggers IO — safe to import anywhere without blocking
  - The initialization order is: `Log.init()` (sync, buffers) → `Global.ensureDirectories()` (creates dirs) → `Log.flush()` (writes buffer to disk)
  - `Global.Path.*` reads are always available immediately — they're just `path.join()` results
  - The cache version check clears the entire cache directory when version mismatches — includes `readdir` + `rm` for each item
  - Test preload uses dynamic `await import()` for both `Log` and `Global` to ensure env vars are set before xdg-basedir reads them
---

## 2026-02-13 - US-006
- Added timeout and graceful degradation for remote config fetching in `config.ts`
- `AbortSignal.timeout(3000)` on all wellknown fetch() calls — 3-second hard timeout
- On timeout/network error: degrade to empty config, log at warning level (no more crash on fetch failure)
- Cache to `Global.Path.cache/wellknown/{xxHash32}.json` with 5-minute TTL
- Stale-while-revalidate: expired cache used immediately, background async refresh
- Converted serial `for` loop to `Promise.allSettled()` — multiple wellknown entries fetched in parallel
- Type narrowing via filter type predicate for wellknown auth entries
- Files changed: config.ts
- **Learnings for future iterations:**
  - `Object.entries(auth).filter(fn)` doesn't narrow union types — need explicit type predicate `(entry): entry is [string, WellknownType]`
  - `Bun.hash.xxHash32()` returns a number — convert with `.toString(16)` for filename-safe hash
  - `AbortSignal.timeout(ms)` creates a signal that auto-aborts after timeout — cleaner than manual AbortController
  - Cache file format: `{ timestamp: number, data: any }` — timestamp used for TTL comparison
  - The `load()` function in config.ts does env var substitution and schema validation — must pass JSON string, not object
---

## 2026-02-13 - US-007
- Parallelized plugin loading in `Plugin.state()` — both internal and external plugins now load via `Promise.allSettled()`
- Internal plugins (CodexAuthPlugin, CopilotAuthPlugin) loaded in parallel
- External plugins: install + import + init all run in parallel per-plugin
- 10-second timeout on `BunProc.install()` via `Promise.race()` to prevent slow npm registry from blocking
- Single plugin failure logged as error but doesn't affect other plugins or startup
- Verified `PluginInput` is read-only (client, project, worktree, directory, serverUrl, $, hasBuiltIn) — safe to share
- Files changed: plugin/index.ts
- **Learnings for future iterations:**
  - `PluginInput` type is defined in `packages/plugin/src/index.ts` — all fields are read-only primitives/objects
  - `BunProc.install()` uses a `Lock.write("bun-install")` — parallel installs will serialize on the lock, which is correct behavior (prevents concurrent bun add)
  - External plugins that return empty string from install (builtin failure) return `[]` hooks — handled by spreading into hooks array
  - `Promise.allSettled()` ensures one plugin timeout/failure doesn't reject the entire batch
  - `Promise.race()` with `setTimeout` for timeout doesn't cancel the underlying install — the lock may still be held. This is acceptable since the timeout only affects the caller
---

## 2026-02-13 - US-008
- Deferred `installDependencies()` out of `Config.state()` critical path — config now returns immediately
- Directories needing install are collected during scan, then installed in background via `Promise.all()`
- On background install completion, `Instance.dispose()` triggers config reload to pick up newly installed plugins
- Added 30-second per-directory timeout via `Promise.race()` to prevent slow npm registry from blocking
- `needsInstall()` check (Option B) already exists — checks `node_modules/@opencode-ai/plugin` existence and version match
- Files changed: config.ts
- **Learnings for future iterations:**
  - `Instance.dispose()` resets lazy state — next `Config.state()` call re-runs the full config loading pipeline
  - Background `Promise.all()` with `Promise.race()` timeout pattern: timeout rejects the race but doesn't cancel the underlying install (bun process continues)
  - The directory scan loop still runs `needsInstall()` synchronously (fast check: `existsSync` + `Bun.file().json()`) — only the actual `installDependencies()` is deferred
  - Plugin/command/agent loading from directories proceeds regardless of install status — existing `node_modules` content is used if available
---

## 2026-02-13 - US-009
- Parallelized three independent operations in `Config.state()`:
  1. `Auth.all()` and `global()` now run via `Promise.all()` — auth reads its own file, global reads config files, no dependency
  2. `Filesystem.findUp("opencode.jsonc")` and `Filesystem.findUp("opencode.json")` run in parallel for project config discovery
  3. `loadCommand()`, `loadAgent()`, `loadMode()`, `loadPlugin()` run via `Promise.all()` per directory — they use different glob patterns on independent subdirectories
- Config precedence order preserved: remote → global → custom → project → directory → inline → managed
- Files changed: config.ts
- **Learnings for future iterations:**
  - `Auth.all()` reads `~/.config/opencode/auth.json` — completely independent of `global()` which reads config files
  - The `global()` result must be merged AFTER remote-fetch results to preserve precedence order (global overrides remote)
  - `Filesystem.findUp` returns files ordered nearest-first — `toReversed()` makes farthest-first for correct low→high merge
  - Original merge order for project config: all jsonc (farthest first), then all json (farthest first) — json at project dir level has highest precedence
---

## 2026-02-13 - US-010
- Parallelized git rev-parse calls were already in place (Promise.all for --show-toplevel and --git-common-dir)
- git rev-list --max-parents=0 --all already only runs on cache miss (existing .git/opencode id file check)
- Added .git/opencode-metadata cache for git rev-parse results with .git/HEAD mtime-based invalidation
- On cache hit (HEAD unchanged): skip both git subprocess spawns entirely — read JSON instead
- On cache miss: run parallel git rev-parse commands and write cache file for next launch (fire-and-forget)
- Files changed: project.ts
- **Learnings for future iterations:**
  - `.git/opencode` stores the root commit ID (project identifier) — separate from `.git/opencode-metadata` which caches rev-parse results
  - `.git/HEAD` mtime changes on checkout, commit, rebase, etc. — reliable invalidation signal for toplevel/commonDir
  - `Bun.file().json()` returns `Promise<any>` — need explicit type annotation on `.then()` callback parameter
  - Fire-and-forget cache writes use `void Bun.file().write().catch(() => undefined)` pattern — same as existing .git/opencode write
  - The cache eliminates 2 subprocess spawns on warm launch — significant on systems where process creation is expensive
---

## 2026-02-13 - US-011
- Parallelized InstanceBootstrap initialization chain into concurrent groups
- Pre-validation confirmed: Plugin.trigger() is NOT called during bootstrap — all 18 call sites are in session runtime code (prompt.ts, llm.ts, processor.ts, bash.ts, pty/index.ts, agent.ts, permission/index.ts, compaction.ts)
- Group A (serial, zero-cost): Share.init(), ShareNext.init(), Snapshot.init(), Truncate.init() — kept serial
- Group B (parallel via Promise.all): Plugin.init(), LSP.init(), Format.init(), SecurityConfig.loadSecurityConfig() — run concurrently
- Group C (parallel via Promise.all, concurrent with B): FileWatcher.init(), File.init(), Vcs.init() — run concurrently with Group B
- All groups B and C run via nested Promise.all() for maximum concurrency
- Files changed: bootstrap.ts
- **Learnings for future iterations:**
  - All init functions in bootstrap are pure setup (register event listeners, initialize state, schedule tasks) — none call Plugin.trigger()
  - Plugin.trigger() is only called in session/runtime modules: prompt.ts (10 calls), llm.ts (2), processor.ts (1), bash.ts (1), pty/index.ts (1), agent.ts (1), permission/index.ts (1), compaction.ts (1)
  - ShareNext.init() was added after the PRD — treated same as Share.init() (Group A, serial)
  - The `measure()` helper records per-phase timing to StartupTrace — works correctly with parallel execution since each call records independently
  - Phase order in the `phases` array reflects declaration order (Group A first, then B, then C) not execution order — timing values are still accurate
---

## 2026-02-13 - US-012
- Created `packages/opencode/script/test-dynamic-import-spike.ts` — comprehensive spike testing 4 dynamic import patterns
- Tested in both interpreted mode (`bun run`) and compiled mode (`bun build --compile`)
- Results:
  - Static string `import("@ai-sdk/anthropic")`: WORKS in both modes (bundler resolves literal string)
  - Map registry `() => import("@ai-sdk/anthropic")`: WORKS in both modes (bundler analyzes arrow function body)
  - Computed variable `import(pkg)`: WORKS in interpreted, FAILS in compiled (bundler can't resolve non-literal specifiers)
  - Import caching (second import of same module): WORKS in both modes (~0ms on subsequent imports)
- Files changed: packages/opencode/script/test-dynamic-import-spike.ts (new)
- **Learnings for future iterations:**
  - Bun compile bundles ALL import() calls with static string literals into the binary — they resolve from `/$bunfs/root/` virtual filesystem
  - Variable-computed import paths get `Cannot find module ... from '/$bunfs/root/spike-test'` — no node_modules at runtime
  - The production build in `script/build.ts` uses `Bun.build()` with `compile` option and `autoloadBunfig: false` — same bundling behavior as `bun build --compile`
  - For US-013: use `Map<string, () => Promise<SDK>>` registry pattern where each value is `() => import("@ai-sdk/pkg")` with literal string
  - Import caching means no need for manual cache — the JS module system already caches after first import()
  - Running spike from `script/` dir (monorepo root) fails because AI SDK deps are in `packages/opencode/node_modules` — must run from packages/opencode or compile from there
  - Compiled binary with `--compile` picks up bunfig.toml in cwd — compile from /tmp or use `autoloadBunfig: false` to avoid preload issues
---

## 2026-02-13 - US-013
- Converted 21 static provider imports in `provider/provider.ts` to lazy dynamic `import()` via `BUNDLED_PROVIDER_LOADERS` Map
- Each entry: `["@ai-sdk/pkg", async () => (await import("@ai-sdk/pkg")).createFn]` — static string literals for Bun compile compatibility
- Updated `getSDK()` to `await` the loader before calling the create function
- Moved `LanguageModelV2` type import from `@openrouter/ai-sdk-provider` to `@ai-sdk/provider` (type-only, no runtime cost)
- Changed `AmazonBedrockProviderSettings` to inline `import()` type syntax in custom loader
- Changed GitLab's `GITLAB_PROVIDER_VERSION` to dynamic `await import("@gitlab/gitlab-ai-provider")` inside the gitlab custom loader
- Changed GitLab's `createGitLab` type reference to `typeof import("@gitlab/gitlab-ai-provider").createGitLab`
- Files changed: provider/provider.ts
- **Learnings for future iterations:**
  - `LanguageModelV2` is NOT exported from `ai` package — it's `LanguageModel` there (which is `GlobalProviderModelId | LanguageModelV2`). Use `@ai-sdk/provider` for the raw `LanguageModelV2` type
  - TypeScript `import type` and inline `import("pkg").Type` syntax are erased at compile time — no runtime module loading
  - JS module system caches `import()` results natively — second `import("@ai-sdk/anthropic")` returns cached module (~0ms)
  - The `BUNDLED_PROVIDER_LOADERS` Map uses `.get()` instead of bracket access — returns `undefined` for missing keys (same behavior as `BUNDLED_PROVIDERS[key]` when key not found)
  - The `@ts-ignore` for `@ai-sdk/github-copilot` is preserved — it maps to the internal `./sdk/copilot` module via dynamic import
---

## 2026-02-13 - US-014
- Deferred LSP.init() and FileWatcher.init() from the awaited bootstrap groups to background pre-warming via queueMicrotask
- LSP.init() moved from Group B (await) to fire-and-forget with .catch() error logging
- FileWatcher.init() moved from Group C (await) to fire-and-forget (already sync internally — calls state() without awaiting)
- Both are pre-warmed immediately after bootstrap completes via queueMicrotask — ready before any user interaction
- Verified: no code depends on LSP/FileWatcher being ready immediately after bootstrap returns
- LSP servers only spawn on-demand in getClients() when a tool invocation requires LSP features
- FileWatcher.init() was already fire-and-forget (state() returns promise but init() doesn't await it)
- Files changed: bootstrap.ts
- **Learnings for future iterations:**
  - `queueMicrotask()` runs after the current task completes but before any I/O callbacks — ideal for non-critical pre-warming
  - LSP.init() returns a Promise (from state() lazy factory) — needs .catch() for fire-and-forget to avoid unhandled rejection
  - FileWatcher.init() is synchronous — calls state() which kicks off async factory internally, returns void
  - The /bootstrap API endpoint only returns timing data — doesn't depend on LSP or FileWatcher being initialized
  - Bus subscriber for Command.Event.Executed (project initialization tracking) doesn't depend on LSP/FileWatcher
  - Vcs.init() subscribes to FileWatcher.Event.Updated but doesn't require FileWatcher to be initialized first — it just registers a listener
---

## 2026-02-13 - US-015
- Evaluated models-snapshot.ts runtime import duration — NO optimization needed
- Benchmark results (1054 KB file, 88 providers):
  - First dynamic import: ~45ms (< 50ms threshold)
  - Subsequent imports: ~12ms (module cache)
  - JSON.parse equivalent: ~3ms
  - Bun.file().json(): ~4ms
- `ModelsDev.Data` already uses `lazy()` — only imported on first access, not during startup
- Load order: file cache → snapshot import → live fetch. In production, cache file is typically populated by refresh() background fetch
- No code changes required — documented finding only
- **Learnings for future iterations:**
  - models-snapshot.ts is ~1MB generated file (in .gitignore) — only exists in production builds
  - `lazy()` from `@/util/lazy` creates a memoized async factory — result cached after first call, `.reset()` clears cache
  - `ModelsDev.refresh()` runs at module level (fire-and-forget) and then every 60min — populates `models.json` cache file
  - In production binary, snapshot is bundled into `$bunfs` — import() reads from virtual filesystem (fast)
  - The ~45ms first import is dominated by JS parsing/eval, not I/O — JSON.parse at ~3ms confirms the 10-15x faster claim
  - No optimization path needed since: (a) under threshold, (b) already lazy, (c) cache file checked first in production
---

## 2026-02-13 - US-016
- Verified TUI first-frame rendering meets <200ms target after all optimizations
- Added `tui-first-frame` timing record in `app.tsx` — captures time from process start to `render()` call
- Created `test/perf/first-frame-verification.test.ts` with 14 tests verifying structural guarantees:
  - resolveNetworkOptions is NOT awaited before tui() in common path
  - getTerminalBackgroundColor uses COLORFGBG fast path → cache → 150ms OSC query (3-tier)
  - getTerminalBackgroundColor caches result for subsequent launches
  - OSC detection timeout is 150ms (not 1000ms)
  - Log.init() is synchronous (no await)
  - global/index.ts has no module-level await
  - tui-first-frame timing recorded before render()
  - worker-spawn timing recorded around Worker creation
  - Non-TUI startup phases complete within budget (models command: global-init + log-init < 100ms)
  - sync.status=loading does NOT prevent message submission (disabled based on permissions/questions)
  - continue flag (-c) waits for session list at partial status
- Measured timing baseline:
  - global-init: ~0ms (no module-level await since US-005)
  - log-init: ~8ms (sync init + ensureDirectories + flush since US-003/US-005)
  - project-detect: ~2ms (cached git metadata since US-010)
  - terminal-bg-detect: 0ms (COLORFGBG or cache since US-002), max 150ms (OSC cold)
  - TUI first-frame: ~10ms cached, ~160ms worst case — well under 200ms target
- Files changed: app.tsx, test/perf/first-frame-verification.test.ts (new)
- **Learnings for future iterations:**
  - `--startup-trace --help` and `--startup-trace --version` do NOT output trace — yargs calls process.exit() before StartupTrace.output()
  - Use `--startup-trace models` for non-interactive timing measurement
  - `StartupTrace.record()` is useful for flat one-shot timing points (like first-frame)
  - Bun test timeout: pass `30000` as third argument to `test()` for longer subprocess tests
  - sync.status has three states: "loading" → "partial" (after blocking API calls) → "complete" (after all data)
  - Prompt disabled state is permission/question-based, NOT loading-state-based
---

## 2026-02-13 - US-017
- Added `test/perf/startup.test.ts` with 12 tests (9 structural + 3 subprocess timing)
- Structural invariant tests (always run):
  - global/index.ts has no module-level await
  - Log.init() is synchronous
  - Provider imports are lazy (BUNDLED_PROVIDER_LOADERS, no static @ai-sdk imports)
  - LSP.init() and FileWatcher.init() deferred from bootstrap (in queueMicrotask)
  - Remote config fetch has AbortSignal.timeout
  - Plugin loading uses Promise.allSettled
  - installDependencies deferred from Config.state() critical path
  - Terminal background detection timeout is 150ms
  - Git metadata caching implemented
- Subprocess timing tests (gated by OPENCODE_PERF_TEST=1):
  - Spawns `bun run src/index.ts --startup-trace models` and parses JSON trace from stderr
  - Validates trace structure (phase names, duration_ms fields)
  - Per-phase thresholds: global-init 75ms, log-init 75ms, project-detect 150ms, git-metadata 150ms, config-load 3000ms, instance-bootstrap 3000ms
  - Non-TUI overhead (global-init + log-init) must be <150ms
- Network isolation via env vars: OPENCODE_DISABLE_MODELS_FETCH, OPENCODE_DISABLE_AUTOUPDATE, OPENCODE_DISABLE_DEFAULT_PLUGINS, OPENCODE_DISABLE_EXTERNAL_SKILLS
- Files changed: test/perf/startup.test.ts (new)
- **Learnings for future iterations:**
  - `test.skipIf(!condition)` in bun:test cleanly skips tests based on env vars — better than wrapping in `if` blocks
  - `Bun.spawn()` returns Process object; use `new Response(proc.stdout).text()` to read stream to string
  - Trace JSON may be mixed with log output in stderr — use regex match `\{[\s\S]*"total_ms"[\s\S]*"phases"[\s\S]*\}` to extract
  - Structural tests are more reliable than timing tests for CI — they verify code invariants that guarantee performance characteristics
  - Per-phase thresholds set to 1.5x baseline are generous enough to avoid flaky tests but tight enough to catch regressions
---
